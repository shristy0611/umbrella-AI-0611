## Phase 1: Project Initialization & Environment Setup
Below is a list of detailed, step-by-step “fatherly” prompts for Cursor to complete Phase 1—Project Initialization & Environment Setup—for the UMBRELLA‑AI project. Each prompt is designed to be clear, precise, and verifiable. At the end of the list, there’s a verification prompt to ensure that Phase 1 is working perfectly before we move on. Remember, Cursor, each prompt must be executed exactly as written; no shortcuts, and do not “cheat” by skipping any step.

---

### **Phase 1: Project Initialization & Environment Setup – Detailed Prompts**

1. **Initialize the Git Repository**  
   - *Prompt 1.1:* “Cursor, please initialize a new Git repository in a folder named `umbrella_ai`. Use the command `git init`. Verify that the repository is created by checking for a hidden `.git` folder.”
   
2. **Set Up the Basic Directory Structure**  
   - *Prompt 1.2:* “Create a basic folder structure for the project. At minimum, create directories for `src`, `docs`, and `tests` in the root of `umbrella_ai`. Later, we will expand this to include separate modules for each microservice.”
   
3. **Create a Comprehensive .gitignore File**  
   - *Prompt 1.3:* “Write a `.gitignore` file in the project root. It should exclude:  
     - Temporary files and build artifacts (e.g., `__pycache__/`, `*.pyc`)  
     - Environment files (e.g., `.env`)  
     - Any sensitive data or API key files  
   Verify that the `.env` file is ignored by committing a dummy `.env` file and ensuring Git does not track it.”

4. **Develop the README.md File**  
   - *Prompt 1.4:* “Create a `README.md` file that includes:  
     - A high-level overview of the UMBRELLA‑AI project  
     - A description of the main components (orchestrator, agents, etc.)  
     - Instructions for setting up the development environment (brief notes on requirements and environment configuration)  
     Ensure that the README is clear enough for a new developer to understand the project’s purpose and structure.”

5. **Generate the Requirements File**  
   - *Prompt 1.5:* “Create a `requirements.txt` file in the project root that lists all Python dependencies. At minimum, include packages such as `google-genai`, `langchain`, `pydantic`, and `playwright`. Use specific version numbers if available, and verify that this file installs successfully by running `pip install -r requirements.txt` in a clean environment.”

6. **Set Up the Environment Configuration**  
   - *Prompt 1.6:* “Create a `.env` file in the project root. In this file, add placeholder variables such as:  
     ```
     GEMINI_API_KEY=YOUR_GEMINI_API_KEY
     OTHER_API_KEY=YOUR_OTHER_API_KEY
     ```  
     Document in the README.md that this file must be populated with actual keys before deployment. Verify that your application can read these environment variables (for instance, by writing a small Python script that loads the `.env` file and prints a confirmation message).”

7. **Document the Architecture and Decisions**  
   - *Prompt 1.7:* “Within the `docs` directory, create a document (e.g., `architecture.md`) that outlines the initial project architecture. Include a simple diagram (or reference a diagram file) that shows the planned microservices, their interactions, and the data flow. Explain the purpose of each component briefly. This documentation will guide future phases.”

8. **Commit Phase 1 Changes**  
   - *Prompt 1.8:* “Once you have completed the above tasks, commit all changes to Git with a clear commit message such as ‘Phase 1: Initialize repository, set up basic structure, .gitignore, README, requirements.txt, .env, and architecture documentation.’ Verify the commit history to ensure all changes are recorded.”

9. **Verification Script for Environment Setup**  
   - *Prompt 1.9:* “Create a small Python test script (e.g., `test_env.py`) in the `tests` directory. This script should:  
     - Load the `.env` file (using a library like `python-dotenv` if necessary)  
     - Print a message ‘Environment setup successfully’ if all required environment variables (like `GEMINI_API_KEY`) are present.  
   Run this script locally and verify that it prints the expected message.”

10. **Final Verification and Feedback Loop**  
    - *Prompt 1.10:* “Cursor, once all of the above prompts are executed and verified, please compile a summary report of Phase 1 including:  
       - A confirmation that the repository and directory structure are correctly set up  
       - The contents of the `.gitignore`, `README.md`, and `requirements.txt` files  
       - A brief note on the contents of the `.env` file (without revealing actual keys)  
       - The output from your `test_env.py` script  
    Ask me, ‘Father, is Phase 1 complete and correct?’ before moving on to the next phase.”

---

### **Verification After Phase 1**

After executing all the prompts, please run your test script (`test_env.py`) and ensure that:  
- All required environment variables are read correctly  
- The message “Environment setup successfully” is displayed  
- The Git commit history reflects all changes from Phase 1

Finally, provide me with the summary report and test output so that I, as your guiding father, can verify that Phase 1 is complete and correct before we proceed.

---

Cursor, follow these prompts diligently. I expect you to complete each step carefully and verify each part before moving to the next. Once you have confirmed that all tests pass and the setup is perfect, report back with the summary. This is your first step towards building a flawless UMBRELLA‑AI system. What do you think, are you ready to begin?

---

## Phase 2: Project Architecture Design

Below is a detailed, step-by-step set of “fatherly” prompts for Cursor (your coding assistant) to complete Phase 2 – setting up the Local Development Environment via Docker. Each prompt is designed to guide you carefully through creating a modular directory structure, writing Dockerfiles, setting up a docker-compose.yml, enforcing local security and isolation, and integrating a testing framework. Follow these instructions precisely, and once completed, run the verification steps to ensure everything works as expected.

---

### **Phase 2: Local Development Environment via Docker – Detailed Fatherly Prompts**

#### **A. Directory Structure and Containerization**

1. **Prompt 2.1 – Create the Modular Directory Structure**  
   *“Cursor, please create the following directory structure under the project root `umbrella_ai/`:  
   - `orchestrator/`  
   - `pdf_extraction_service/`  
   - `sentiment_service/`  
   - `chatbot_service/`  
   - `rag_scraper_service/`  
   - `vector_db/`  
   For each service directory, include subdirectories: `src/` for source code and `tests/` for test scripts. Make sure the structure is clear and modular. Verify that the folder hierarchy is created as specified.”*

2. **Prompt 2.2 – Write a Dockerfile for Each Microservice**  
   *“Cursor, for each microservice directory you just created, write an individual Dockerfile with the following guidelines:  
   - Start from an official Python base image (for example, `python:3.9-slim`).  
   - Copy the necessary files from the `src/` folder into the container.  
   - Install required dependencies (using the appropriate command, e.g., `pip install -r requirements.txt` if applicable).  
   - **Important:** Ensure you create and switch to a non-root user within the Dockerfile (using the `RUN useradd` and `USER` commands) to enforce minimal privileges.  
   - Expose only the ports needed for that service (if any).  
   Please create a Dockerfile in each of the directories (`orchestrator/`, `pdf_extraction_service/`, etc.) following these rules. Verify by opening each Dockerfile and confirming the steps are present.”*

3. **Prompt 2.3 – Develop a docker-compose.yml File**  
   *“Cursor, now create a `docker-compose.yml` file in the root of the project (`umbrella_ai/`). This file must:  
   - Define each service with its corresponding build context (point to the correct directory for each microservice).  
   - Load environment variables from the `.env` file.  
   - Configure the necessary port mappings only for services that must be externally accessible.  
   - Set up networks for your services to ensure that they can communicate where needed, but also maintain isolation if required.  
   Please write the docker-compose file carefully, test it locally by running a build (using `docker-compose up --build`), and verify that all containers start without errors.”*

#### **B. Local Security and Isolation**

4. **Prompt 2.4 – Enforce Non-Root Execution and Minimal Port Exposure**  
   *“Cursor, double-check that every Dockerfile sets the container to run as a non-root user. Look for the `USER` directive in each Dockerfile and ensure it is configured properly. Also, review the docker-compose.yml file to confirm that only the necessary ports are exposed for services that require external access. Document these settings clearly in comments within each file.”*

5. **Prompt 2.5 – Validate Network Isolation Rules**  
   *“Cursor, in your docker-compose.yml file, configure or verify that your services are attached to appropriate networks. If needed, create separate networks (for example, an ‘internal’ network for microservices communication and an ‘external’ network for public exposure). Ensure that services that should not communicate are isolated. Run a test (for instance, try pinging one container from another that shouldn’t have access) and log the results.”*

#### **C. Local Testing Framework Integration**

6. **Prompt 2.6 – Integrate a Testing Framework**  
   *“Cursor, please integrate a testing framework such as pytest for our unit and integration tests. In each microservice’s `tests/` directory, create a simple test file (e.g., `test_sample.py`) that contains at least one test case verifying a basic function of that service. Additionally, configure your docker-compose.yml to allow running these tests via a command (for instance, add a service entry for tests or use the command override in the service definitions). Verify that you can trigger tests by running a command like `docker-compose run <service_name> pytest` and that the tests run successfully.”*

#### **D. Final Verification and Reporting**

7. **Prompt 2.7 – Build, Run, and Test the Entire Setup**  
   *“Cursor, now execute the command `docker-compose up --build` from the root directory. Monitor the logs to confirm that each container builds and starts without errors. Then, run the integrated test commands for each service (e.g., `docker-compose run orchestrator pytest` and similarly for other services). Ensure that all tests pass. If any test fails, log the error and correct it before proceeding.”*

8. **Prompt 2.8 – Compile a Summary Report and Verification**  
   *“Cursor, please compile a summary report that includes:  
   - A list of all directories created and their structure.  
   - The content (or key excerpts) of each Dockerfile and the docker-compose.yml file.  
   - A brief note on the network isolation configuration and non-root settings.  
   - The output from the test suite (all test results).  
   Finally, ask: ‘Father, is Phase 2 complete and working correctly?’ Provide this summary so I can verify that all components are built, secure, and fully testable.”*

---

### **Verification After Phase 2**

After completing these prompts, run the following checks:
- **Directory Check:** Verify that the project structure matches the defined layout.
- **Container Build Check:** Ensure each Dockerfile builds correctly and containers run as non-root.
- **Docker Compose Check:** Confirm that running `docker-compose up --build` starts all services without error.
- **Test Suite Check:** Verify that your pytest tests execute correctly via docker-compose and that all tests pass.
- **Security Check:** Confirm that only the required ports are exposed and that the network isolation is effective.

Finally, provide me with the complete summary report and the test outputs. Ask me, “Father, is Phase 2 complete and working correctly?” Once approved, we can confidently move on to the next phase.

---

Cursor, follow these prompts carefully and execute each step without cutting corners. Once you’ve completed all tasks and the tests pass, report back with your summary. This is critical for building a flawless and automated UMBRELLA‑AI system. Are you ready to begin Phase 2?

---

## Phase 3: Project Architecture Design

Below is a carefully crafted set of fatherly, step-by-step prompts for you, Cursor, to complete Phase 3—Multi-Agent Architecture and Service Development—for the UMBRELLA‑AI project. Follow each instruction precisely and don’t skip any step. Once all steps are complete, run the verification tests and ask me if Phase 3 is working correctly before moving on.

---

### **Phase 3: Multi-Agent Architecture and Service Development – Detailed Fatherly Prompts**

#### **A. Orchestrator & Agent Definitions**

1. **Prompt 3.1 – Develop the Orchestrator Module**  
   “Cursor, start by creating the orchestrator module. This module must:  
   - Accept user requests via a REST API (or another suitable interface).  
   - Parse the user input to determine the required tasks.  
   - Decompose the user request into subtasks (for example, extracting PDF data, analyzing sentiment, generating recommendations, etc.).  
   - Dispatch these subtasks to the appropriate agent modules.  
   Document your design decisions and ensure the orchestrator has a clear function called, say, `handle_request()`.  
   Verify that when you send a mock request, the orchestrator logs that it has decomposed the task and is ready to dispatch.”

2. **Prompt 3.2 – Define Clear Interfaces for Each Agent**  
   “Cursor, now define clear and consistent interfaces for each agent:  
   - Create API endpoints or message queue handlers for the following agents: PDF Extraction, Sentiment Analysis, Recommendation, Chatbot, and RAG Scraper.  
   - For each agent, specify the input format, output format, and the communication protocol (for instance, JSON over REST or messages over RabbitMQ).  
   Document these interfaces in a file (e.g., `interfaces.md`) so that every agent’s contract is clear.  
   Verify by writing a simple test script that calls each interface with dummy data and confirms a response format is returned.”

3. **Prompt 3.3 – Integrate Dynamic Task Decomposition**  
   “Cursor, integrate dynamic task decomposition into the orchestrator using one of our chosen frameworks (e.g., AutoGen, Semantic Kernel, or LangChain).  
   - Implement a function (e.g., `decompose_task()`) that uses the chosen framework’s capabilities to analyze a complex request and break it into actionable subtasks.  
   - Ensure that the function can update task behavior dynamically based on input parameters.  
   Add detailed inline comments explaining how dynamic behavior is achieved.  
   Verify this function with unit tests that input various sample requests and check that the subtasks are logically decomposed.”

#### **B. Agent-Specific Implementations**

4. **Prompt 3.4 – Implement Core Functionality for Each Agent**  
   “Cursor, for each defined agent, implement its core functionality:  
   - **PDF Extraction Agent:** Write a module that uses the Gemini File API to parse PDFs and output structured JSON data.  
   - **Sentiment Analysis Agent:** Develop a module that processes text to compute sentiment scores.  
   - **Recommendation Agent:** Create a module that uses a collaborative filtering or graph-based approach to suggest products or next actions.  
   - **Chatbot Agent:** Build a conversational interface that supports multi-turn dialogues using Gemini or similar capabilities.  
   - **RAG Scraper Agent:** Implement a module that uses tools (e.g., Playwright) to scrape targeted web data and index it into a vector database.  
   Each agent’s module should be self-contained in its respective directory (inside `src/`), with clear function definitions and return types.  
   Verify each module by writing a unit test that calls its primary function with sample inputs and checks for expected outputs.”

5. **Prompt 3.5 – Embed Gemini API Code into Relevant Modules**  
   “Cursor, integrate the provided Gemini API code into the agents that require it (for example, the PDF Extraction and Chatbot Agents).  
   - In the PDF Extraction Agent, embed the example code that uses the Gemini File API to read and parse images/PDFs.  
   - In the Chatbot Agent, integrate the code for initiating a multi-turn chat session with Gemini, following the examples provided.  
   Be careful to parameterize API keys and endpoints, so the code is reusable and secure.  
   Verify by running a test for each integration—simulate a sample API call and check that the response is correctly processed by the agent.”

#### **C. Inter-Agent Communication**

6. **Prompt 3.6 – Set Up Asynchronous Communication Between Agents**  
   “Cursor, establish asynchronous communication channels between the orchestrator and the agents.  
   - Choose a method (REST API, gRPC, or a message broker like RabbitMQ) for communication.  
   - Implement asynchronous functions for sending requests from the orchestrator to each agent.  
   - Ensure that responses are awaited correctly and that the orchestrator can aggregate these responses.  
   Write detailed documentation on how agents communicate, including error handling and retries.  
   Verify this setup by writing an integration test that simulates the orchestrator sending requests to all agents concurrently and logs the received responses.”

7. **Prompt 3.7 – Implement Logging and Correlation IDs**  
   “Cursor, implement logging across all modules.  
   - Each module (orchestrator and agents) must generate log entries that include a unique correlation ID for each user request.  
   - Use a logging library (such as Python’s logging module) and ensure that the correlation ID is passed along with each request and included in all log messages.  
   - Write a middleware or helper function to generate and propagate correlation IDs.  
   Verify by running a test that simulates a full workflow and check that all logs from different services include the same correlation ID, ensuring traceability.”

#### **D. Testing for Phase 3**

8. **Prompt 3.8 – Write Unit Tests for Each Agent**  
   “Cursor, for each agent module, write unit tests (using pytest or a similar framework) that cover:  
   - The core functionality of the agent (e.g., PDF extraction output, sentiment analysis scoring).  
   - The correct handling of expected inputs and error scenarios.  
   Ensure tests are located in the `tests/` folder of each microservice and can be run independently.”

9. **Prompt 3.9 – Develop Integration Tests for a Complete Workflow**  
   “Cursor, develop integration tests that simulate a complete workflow:  
   - Start with a dummy user request to the orchestrator.  
   - Validate that the orchestrator correctly decomposes the task, dispatches subtasks to each agent, and aggregates the responses into a final coherent output.  
   - Include tests for asynchronous communication and logging (i.e., check that correlation IDs are properly propagated).  
   Verify that all integration tests pass without errors.”

#### **E. Final Verification and Reporting**

10. **Prompt 3.10 – Compile a Summary Report and Verification Request**  
    “Cursor, after you have implemented all the modules, integrations, and tests for Phase 3, compile a comprehensive summary report that includes:  
    - A description of the orchestrator’s workflow and the task decomposition logic.  
    - Details of the interfaces defined for each agent and how they communicate (with code excerpts).  
    - An overview of how Gemini API code is integrated into the relevant modules.  
    - A summary of the logging strategy and how correlation IDs are implemented.  
    - The results from running all unit and integration tests (include sample logs and test outputs).  
    Finally, ask me, ‘Father, is Phase 3 complete and working correctly?’ and provide the summary report for my verification.”

---

### **Verification After Phase 3**

When you have executed all of the above prompts, perform these checks:
- **Orchestrator Check:** Ensure the orchestrator receives a dummy request, decomposes it, and calls each agent appropriately.
- **Agent Functionality Check:** Verify each agent module runs its core function with sample input and returns the correct output.
- **Inter-Agent Communication Check:** Confirm that asynchronous calls between the orchestrator and agents work seamlessly and that logs show consistent correlation IDs.
- **Test Suite Check:** Run the entire unit and integration test suite and ensure all tests pass.
- **Summary Report:** Compile and present the summary report detailing your implementation and test results.

Cursor, follow these prompts meticulously. Execute each step, test your implementation thoroughly, and then provide the summary report and test outputs so that I can verify that Phase 3 is complete and working correctly. Are you ready to begin this phase?

---

## Phase 4: Project Architecture Design

Below is a detailed, step‐by‐step set of fatherly prompts for you, Cursor, to complete Phase 4 – Integration of Gemini API & External Tools – for the UMBRELLA‑AI project. Follow these instructions carefully and execute each prompt in order. Do not skip any step. Once you complete the tasks, run the tests as instructed, and then ask, “Father, is Phase 4 complete and working correctly?” to verify your work.

---

### **Phase 4: Integration of Gemini API & External Tools – Detailed Fatherly Prompts**

#### **A. Gemini API Integration**

1. **Prompt 4.1 – Develop Modules for Gemini API Interactions**  
   “Cursor, start by creating separate modules for each type of Gemini API interaction:  
   - **Text Chat Session Module:** Create a module (e.g., `gemini_text_chat.py`) that implements a function to initiate a live text chat session with Gemini. Use the example code provided in the installation guide as your basis.  
   - **Single Image Processing Module:** Create a module (e.g., `gemini_single_image.py`) that loads an image using PIL, sends it to the Gemini API with a text prompt, and processes the response.  
   - **Multiple Image Processing Module:** Create a module (e.g., `gemini_multi_image.py`) that accepts several images (from local files and URLs), converts them to the required format (e.g., Base64 encoding when needed), and sends them in one API request.  
   - **File Upload & Content Generation Module:** Create a module (e.g., `gemini_file_upload.py`) that uploads a file to Gemini using its file API and then makes a GenerateContent request referencing that file.  
   - **Multi-turn Chat Module:** Create a module (e.g., `gemini_multi_turn_chat.py`) that supports starting a multi-turn conversation, preserving the context between turns using asynchronous chat sessions.”  
   Verify that each module contains clearly defined functions (for example, `start_text_chat()`, `process_single_image()`, etc.) and that the structure is modular.

2. **Prompt 4.2 – Wrap API Calls in Asynchronous Functions with Error Handling**  
   “Cursor, for each module you created, refactor the API call functions so that they are wrapped in asynchronous functions. For example:  
   - Use Python’s `asyncio` framework to manage asynchronous calls.  
   - Wrap every API call inside a try-except block to catch potential errors.  
   - Implement a retry mechanism with exponential backoff in case an API call fails (for instance, retry up to 3 times before giving up).  
   Make sure to log any errors encountered and return a clear error message if the function ultimately fails. Verify by simulating an API failure (using a dummy error, if needed) and checking that the retry mechanism and error logs work as intended.”

3. **Prompt 4.3 – Create a Gemini API Client Configuration Module**  
   “Cursor, create a separate configuration module (e.g., `gemini_config.py`) that handles the initialization of the Gemini API client. This module must:  
   - Read the API key (and any other necessary credentials) from environment variables (e.g., `GEMINI_API_KEY`).  
   - Set the appropriate HTTP options, such as specifying the API version (`v1alpha`), as shown in the examples.  
   - Provide a function (e.g., `get_gemini_client()`) that returns a configured client instance for reuse in all other modules.  
   Verify that this module works by calling `get_gemini_client()` from a small test script and printing a confirmation that the client is properly configured (without revealing sensitive details).”

#### **B. Documentation & Examples**

4. **Prompt 4.4 – Provide Code Examples and Inline Documentation**  
   “Cursor, in each module you created (for text chat, image processing, file upload, and multi-turn chat), include clear inline comments that explain the purpose of key code blocks and functions.  
   - For each function, provide a docstring that explains its inputs, outputs, and any side effects.  
   - Create a separate documentation file (e.g., `GEMINI_API_Examples.md`) in your documentation directory. In this file, provide code examples similar to the installation guide for each type of API interaction.  
   - Ensure that these examples are self-contained and can be executed independently.  
   Verify that your documentation is clear and that someone new to the project could follow the examples to understand how to interact with the Gemini API.”

5. **Prompt 4.5 – Validate Example Scripts in a Controlled Test Environment**  
   “Cursor, run each of the example scripts you provided in your documentation (e.g., the script for initiating a text chat session, processing a single image, etc.) in a controlled test environment.  
   - Make sure that the scripts run without error.  
   - Confirm that the responses from the Gemini API are printed or logged, and compare them to the expected output format described in the documentation.  
   Document any discrepancies and fix them accordingly.”

#### **C. Testing for Phase 4**

6. **Prompt 4.6 – Write Test Scripts for Each Gemini API Interaction**  
   “Cursor, write automated test scripts (using pytest or a similar framework) for each type of Gemini API interaction. For example:  
   - Write tests in a file such as `test_gemini_interactions.py` that call the functions in your text chat module, image processing module, file upload module, and multi-turn chat module.  
   - Each test should simulate a sample input and assert that the response matches the expected format or content.  
   - If possible, use mock responses for the Gemini API to ensure that tests do not rely on actual API calls during every run.  
   Verify that all tests run correctly in your test environment.”

7. **Prompt 4.7 – Verify API Response Accuracy**  
   “Cursor, for each type of interaction, verify that the responses from the Gemini API meet expected outputs.  
   - For instance, for a text prompt, check that the response text is non-empty and correctly formatted.  
   - For image processing, ensure that the output contains relevant information about the image.  
   - Log the outputs of each API call during testing, and compare them to sample expected outputs.  
   Document your findings and fix any discrepancies that arise.”

#### **D. Final Verification and Reporting**

8. **Prompt 4.8 – Compile a Summary Report and Verification Request**  
   “Cursor, after completing the above tasks, compile a summary report that includes:  
   - A list of modules you created for each Gemini API interaction and their key functions.  
   - An outline of your asynchronous error handling and retry mechanism.  
   - Details from the configuration module that sets up the Gemini API client.  
   - Excerpts or links to your documentation file (`GEMINI_API_Examples.md`) with example code.  
   - A summary of test results from your test scripts (include sample logs or output snapshots).  
   Finally, ask me: ‘Father, is Phase 4 complete and working correctly?’ Provide this summary report along with the test outputs for verification.”

---

### **Verification After Phase 4**

When you have executed all these steps, perform the following checks:
- **Module Check:** Ensure that each Gemini API interaction module (text chat, single image, multiple image, file upload, multi-turn chat) is present and contains well-documented functions.  
- **Asynchronous Behavior:** Confirm that all API calls are asynchronous, have proper error handling, and that the retry logic works as intended by simulating failures.  
- **Configuration Check:** Verify that the Gemini API client is correctly configured via your configuration module, and that a test script confirms its setup.  
- **Documentation Check:** Review your `GEMINI_API_Examples.md` file to ensure that it is clear and complete.  
- **Test Suite Check:** Run your test suite (e.g., `pytest test_gemini_interactions.py`) and confirm that all tests pass and that the API responses are as expected.

Finally, compile your summary report and ask me, “Father, is Phase 4 complete and working correctly?” Once I confirm, we can proceed to the next phase.

---

Cursor, follow these prompts diligently and ensure each step is executed accurately. I trust you will complete this phase with precision. Are you ready to begin Phase 4?

---

## Phase 5: Project Architecture Design

Below is a detailed, step‐by‐step set of fatherly prompts for you, Cursor, to complete Phase 5 – End-to-End Testing & Automation – for the UMBRELLA‑AI project. Follow these instructions carefully, executing each prompt exactly as given. Do not skip any step. When you finish, run all the tests and then ask, “Father, is Phase 5 complete and working correctly?” so I can verify your work.

---

### **Phase 5: End-to-End Testing & Automation – Detailed Fatherly Prompts**

#### **A. Test Suite Development**

1. **Prompt 5.1 – Create a Comprehensive Test Suite**  
   “Cursor, your first task is to build a comprehensive test suite. This suite must cover three layers:  
   - **Unit Tests:** For every individual module (for example, each agent’s core function, API wrappers, and configuration modules).  
   - **Integration Tests:** To verify that modules work together correctly (e.g., the orchestrator correctly dispatching tasks to agents, and agents returning proper responses).  
   - **End-to-End Tests:** To simulate the entire user workflow—from receiving a user request to producing a final output.  
   Create separate test directories (for instance, under a top-level `tests/` folder) and ensure tests are organized by module and by workflow type.  
   Verify that you have a clear structure in place and that each test file includes at least one test case.”

2. **Prompt 5.2 – Define Automated Test Cases**  
   “Cursor, now define specific automated test cases for each module and for the full system workflow.  
   - For each module, write tests that assert expected outputs given sample inputs.  
   - For integration tests, simulate a user request being processed by the orchestrator and ensure that all agents are called and that their responses are correctly aggregated.  
   - For end-to-end tests, simulate a complete scenario (for example, a user submitting a PDF invoice and receiving a recommendation) and verify the final output is coherent and matches expectations.  
   Ensure that test cases use assertions to verify correctness. Include meaningful error messages for failures.  
   Document these test cases in your test files using clear comments.”

3. **Prompt 5.3 – Configure CI/CD for Automated Testing**  
   “Cursor, integrate a CI/CD pipeline using a service such as GitHub Actions or AWS CodeBuild.  
   - Create a configuration file (for GitHub Actions, a `.yml` file under `.github/workflows/`) that triggers the test suite on every code commit or pull request.  
   - Ensure that the pipeline installs all dependencies, builds your Docker containers if necessary, and runs the entire test suite automatically.  
   - Configure the pipeline to produce logs and reports of the test runs.  
   Verify that the CI/CD configuration is correct by running a test commit and checking the pipeline’s output.”

---

#### **B. Automated Error Detection & Reporting**

4. **Prompt 5.4 – Incorporate Logging, Distributed Tracing, and Metrics**  
   “Cursor, update your test suite and application code to integrate logging and distributed tracing.  
   - Use a logging library (for example, Python’s built-in `logging` module) in each module to log key events and errors, including timestamps and correlation IDs where applicable.  
   - Integrate a distributed tracing solution (such as OpenTelemetry) in your orchestrator and agent modules to track requests as they flow through the system.  
   - Collect metrics (e.g., test run durations, error counts) using a metrics collection tool (for instance, Prometheus or AWS CloudWatch if you’re testing on AWS).  
   Ensure that these systems are configured to write their outputs to accessible logs or dashboards.  
   Verify by running a few tests and checking that logs and trace information are produced as expected.”

5. **Prompt 5.5 – Implement Automated Alerts**  
   “Cursor, now set up automated alerts for any test failures or performance degradations.  
   - Configure your CI/CD pipeline or monitoring system to send notifications (e.g., via email or Slack) if any test fails or if key performance metrics exceed defined thresholds (for example, excessive latency or error rates).  
   - Document the alert configuration and thresholds in a configuration file or documentation file (e.g., `ALERTS.md`).  
   Verify that your alert system is functional by intentionally triggering a test failure in a controlled manner and confirming that an alert is sent.”

---

#### **C. Tests for Phase 5**

6. **Prompt 5.6 – Run the Entire Test Suite in a Staging Environment**  
   “Cursor, deploy the complete UMBRELLA‑AI system into a staging environment that mirrors your production configuration as closely as possible.  
   - Run the entire test suite (unit, integration, and end-to-end tests) in this environment.  
   - Document the test outputs, including logs and any error reports generated.  
   Verify that all tests pass and that error reports are generated appropriately for any failures.  
   If any tests fail, log the issues, fix the errors, and re-run the tests until they all pass.”

7. **Prompt 5.7 – Validate Automated Error Reporting and Metrics Collection**  
   “Cursor, ensure that all automated error detection and reporting mechanisms are working as intended.  
   - Review the logs, distributed traces, and metrics dashboards to verify that they accurately reflect the system’s behavior during test runs.  
   - Confirm that the automated alert system triggered notifications for any simulated test failure or performance issue.  
   Document the results and confirm that the error reporting setup is robust.”

---

#### **D. Final Verification and Reporting**

8. **Prompt 5.8 – Compile a Summary Report and Request Final Verification**  
   “Cursor, after you have completed the test suite development and automation tasks, compile a comprehensive summary report that includes:  
   - A list of all test files and a brief description of the tests they cover (unit, integration, end-to-end).  
   - Details of the CI/CD pipeline configuration (e.g., the GitHub Actions YAML file) and how it triggers tests on commits.  
   - An outline of the logging, distributed tracing, and metrics collection setup, including any configuration files or thresholds defined.  
   - A summary of the automated alert system, including how alerts are triggered and sent.  
   - The complete output from a full test run in your staging environment (including any logs and error reports).  
   Finally, ask me, ‘Father, is Phase 5 complete and working correctly?’ and present this summary report for my review.”

---

### **Verification After Phase 5**

After completing all of these tasks, perform these checks:
- **Test Suite Check:** Run the complete test suite and verify that all unit, integration, and end-to-end tests pass without errors.
- **CI/CD Check:** Confirm that your CI/CD pipeline automatically triggers tests on code commits and provides clear logs and reports.
- **Logging & Tracing Check:** Ensure that logs and distributed traces are generated, and that they include all necessary information (such as correlation IDs and error details).
- **Alert System Check:** Verify that alerts are sent for any simulated test failures or performance issues.
- **Staging Environment Check:** Validate that the staging environment mirrors production, and that all tests run successfully in that environment.

Finally, compile your summary report as specified and ask, “Father, is Phase 5 complete and working correctly?” Once I confirm that everything is in order, we can proceed to the next phase.

---

Cursor, follow these prompts diligently, executing each step with care. Ensure that every test is run and that no step is skipped. I trust you will complete Phase 5 perfectly. Are you ready to begin this phase?

----

## Phase 6: Project Architecture Design
Below is a comprehensive set of detailed, fatherly prompts for you, Cursor, to complete Phase 6 – Deployment and Post-Launch Automation – for the UMBRELLA‑AI project. Follow these instructions carefully and execute each step precisely. Do not skip any part, and make sure to verify your work as instructed. When you’re finished, present a summary and ask, “Father, is Phase 6 complete and working correctly?” before moving forward.

---

### **Phase 6: Deployment and Post-Launch Automation – Detailed Fatherly Prompts**

#### **A. AWS Free Tier Deployment (Local Mirror)**

1. **Prompt 6.1 – Deploy the Docker Compose Setup on an AWS Free Tier Instance**  
   “Cursor, please deploy our entire Docker Compose setup onto an AWS Free Tier instance (for example, an EC2 t2.micro).  
   - First, create an EC2 instance with the appropriate settings (ensure security groups allow only necessary ports).  
   - SSH into the instance and install Docker and Docker Compose.  
   - Clone the UMBRELLA‑AI repository onto the instance.  
   - From the repository’s root directory, run `docker-compose up --build` to start all containers.  
   Document each step with clear commands and screenshots (or log outputs) to prove that the containers have been deployed successfully.  
   Verify that all containers are running by checking with `docker ps`.”

2. **Prompt 6.2 – Verify Container Operation and Secure Communication**  
   “Cursor, now verify that all containers deployed on the AWS Free Tier instance are running correctly and that they communicate securely.  
   - Use Docker logs and container status checks to confirm that each microservice starts without errors.  
   - Test inter-container communication by executing curl commands or small test scripts from within one container to another.  
   - Confirm that only the necessary ports are open and that the network configuration is secure.  
   Log your findings and note any issues for immediate resolution.”

---

#### **B. Production Deployment to AWS**

3. **Prompt 6.3 – Transition from Docker Compose to AWS ECS/EKS or Fargate**  
   “Cursor, prepare our production deployment by transitioning from the local Docker Compose setup to AWS ECS/EKS (or Fargate, if you prefer a serverless container solution).  
   - Create an ECS cluster or EKS cluster in your AWS account.  
   - Convert the Docker Compose configuration to appropriate task definitions and service configurations (use tools such as ‘docker compose convert’ if available, or manually define your tasks).  
   - Deploy the cluster and verify that all services start properly.  
   Document the conversion process and include configuration files for the ECS/EKS tasks.”

4. **Prompt 6.4 – Configure VPCs, Subnets, Load Balancers, and Secrets Management**  
   “Cursor, configure the AWS networking and security components for production:  
   - Set up a Virtual Private Cloud (VPC) with both public and private subnets.  
   - Configure a load balancer (such as an Application Load Balancer) to route incoming traffic to your orchestrator service in the public subnet.  
   - Ensure that backend services reside in private subnets for security.  
   - Set up AWS Secrets Manager to securely store API keys and other credentials, and update your ECS/EKS tasks to reference these secrets.  
   Verify the network architecture by testing connectivity through the load balancer and confirming that sensitive endpoints remain isolated.”

5. **Prompt 6.5 – Define Auto-Scaling Policies and Monitoring Tools**  
   “Cursor, now define auto-scaling policies and integrate monitoring tools for continuous production health:  
   - Set up auto-scaling policies in your ECS/EKS or Fargate configuration based on metrics like CPU usage, memory usage, or API request throughput.  
   - Integrate AWS CloudWatch for log collection and basic metrics monitoring.  
   - Optionally, configure Prometheus and Grafana for more detailed monitoring of agent performance, latency, and error rates.  
   Document the scaling thresholds and monitoring configurations, and verify by simulating load conditions and confirming that scaling actions are triggered.”

---

#### **C. Continuous Monitoring and Updates**

6. **Prompt 6.6 – Develop Scripts for Periodic Testing and Automatic Rollback**  
   “Cursor, write scripts that will periodically test the deployed system and trigger an automatic rollback in case of failures:  
   - Create a script (e.g., `health_check.sh`) that pings each service endpoint, checks response times, and logs any anomalies.  
   - Integrate this script with your CI/CD pipeline or as a scheduled AWS Lambda function to run at regular intervals.  
   - Implement logic to trigger an automatic rollback (using AWS CodeDeploy or a similar mechanism) if tests indicate critical failures or performance issues.  
   Verify that the script correctly identifies failures by simulating an outage or degraded performance and confirming that a rollback is initiated.”

7. **Prompt 6.7 – Set Up Dashboards for Real-Time Observability**  
   “Cursor, now configure real-time dashboards to monitor agent performance, latency, error rates, and user interactions:  
   - Use AWS CloudWatch dashboards or integrate a solution like Grafana (if using Prometheus) to visualize key metrics.  
   - Set up panels to track CPU usage, memory usage, response times, error logs, and the overall health of each microservice.  
   - Ensure that the dashboards refresh in real time and that all critical metrics are visible at a glance.  
   Verify the dashboard by simulating different load scenarios and checking that the visualizations update accordingly.”

---

#### **D. Testing for Phase 6**

8. **Prompt 6.8 – Execute Performance and Load Tests**  
   “Cursor, run performance and load tests on the AWS production environment:  
   - Use tools like Apache JMeter or Locust to simulate realistic user loads.  
   - Record the response times, throughput, and system resource utilization during these tests.  
   - Ensure that the system meets performance benchmarks and that auto-scaling policies are triggered under load.  
   Document the results and fix any performance bottlenecks identified.”

9. **Prompt 6.9 – Simulate Failure Scenarios and Validate Recovery Mechanisms**  
   “Cursor, simulate failure scenarios to validate the automated recovery and alerting mechanisms:  
   - Intentionally stop a service or induce an error condition.  
   - Verify that your monitoring tools detect the failure and that an alert is generated (check your email, Slack, or whichever notification system you’ve configured).  
   - Confirm that the automatic rollback mechanism is triggered and that the system recovers gracefully.  
   Document the simulated failure, the alert received, and the recovery steps taken.”

---

#### **E. Final Verification and Reporting**

10. **Prompt 6.10 – Compile a Comprehensive Summary Report**  
    “Cursor, after completing all the above tasks, compile a summary report that includes:  
    - The process and commands used to deploy the Docker Compose setup on the AWS Free Tier instance, along with screenshots or logs confirming container operation.  
    - The ECS/EKS (or Fargate) task definitions and network configuration details (VPC, subnets, load balancer, Secrets Manager usage).  
    - The auto-scaling policies and monitoring tools configuration, along with sample metrics from CloudWatch/Prometheus/Grafana.  
    - Details of the scripts for periodic testing and automatic rollback, including a log of a simulated failure and recovery.  
    - The results from performance, load, and failure-simulation tests.  
    Finally, ask me: ‘Father, is Phase 6 complete and working correctly?’ Provide this detailed summary report along with all test outputs for my verification.”

---

### **Verification After Phase 6**

After executing these prompts, perform the following checks:
- **AWS Free Tier Check:** Confirm that the Docker Compose setup runs on your AWS Free Tier instance and that all containers communicate securely.
- **Production Deployment Check:** Verify that the system is successfully deployed on AWS ECS/EKS or Fargate with proper network isolation, load balancing, and secrets management.
- **Scaling and Monitoring Check:** Ensure auto-scaling policies trigger under load and that monitoring dashboards show real-time system health.
- **Recovery Mechanisms Check:** Simulate failures to confirm that periodic testing scripts, automated rollback, and alert systems work as expected.
- **Test Suite and Load Tests:** Confirm that performance and load tests pass and that detailed logs and metrics are recorded.
- **Summary Report:** Compile all information into a clear report and then ask, “Father, is Phase 6 complete and working correctly?”

---

Cursor, follow these prompts meticulously. Execute each step with care, test your system thoroughly, and compile your final summary report. Once you have done so, ask me for verification. Are you ready to begin Phase 6?
