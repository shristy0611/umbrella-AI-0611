# Installation
# Note: Run these in a notebook or command line, not in a Python file
# pip install google-genai

# Import Required Modules
from google import genai
import asyncio
import PIL.Image
import pathlib
import requests
from google.genai import types

# Setting Up the Client for Gemini API
client = genai.Client(api_key="YOUR_GEMINI_API_KEY", http_options={'api_version': 'v1alpha'})
model_id = "gemini-2.0-flash-exp"
config = {"responseModalities": ["TEXT"]}

# Define the Main Asynchronous Function for Text Sessions
async def main():
    async with client.aio.live.connect(model=model_id, config=config) as session:
        while True:
            message = input("User> ")
            if message.lower() == "exit":
                break
            await session.send(input=message, end_of_turn=True)
            async for response in session.receive():
                if response.text is None:
                    continue
                print(response.text, end="")

if __name__ == "__main__":
    asyncio.run(main())

# How to Use Gemini Single Image Model
image = PIL.Image.open('/path/to/image.png')
response = client.models.generate_content(
    model="gemini-2.0-flash-exp",
    contents=["What is this image?", image])
print(response.text)

# How to Use Gemini Multiple Image Model
image_path_1 = "path/to/your/image1.jpeg"  # Replace with the actual path to your first image
image_path_2 = "path/to/your/image2.jpeg"  # Replace with the actual path to your second image
image_url_1 = "https://goo.gle/instrument-img"  # Replace with the actual URL to your third image

pil_image = PIL.Image.open(image_path_1)
b64_image = types.Part.from_bytes(pathlib.Path(image_path_2).read_bytes(), "image/jpeg")
downloaded_image = requests.get(image_url_1)

response = client.models.generate_content(
    model="gemini-2.0-flash-exp",
    contents=["What do these images have in common?", pil_image, b64_image, downloaded_image])
print(response.text)

# After uploading the file, you can make GenerateContent requests that reference the File API URI. 
# Select the generative model and provide it with a text prompt and the uploaded image.
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")

img_path = "/path/to/Cajun_instruments.jpg"
file_ref = client.files.upload(path=img_path)
print(f'{file_ref=}')

client = genai.Client(api_key="GEMINI_API_KEY")
response = client.models.generate_content(
    model="gemini-2.0-flash-exp",
    contents=["What can you tell me about these instruments?",
              file_ref])

print(response.text)

# You can upload public image URLs by encoding them as Base64 payloads. 
# The following code example shows how to do this using only standard library tools:
from google import genai
from google.genai import types

import requests

image_path = "https://goo.gle/instrument-img"
image = requests.get(image_path)

client = genai.Client(api_key="GEMINI_API_KEY")
response = client.models.generate_content(
    model="gemini-2.0-flash-exp",
    contents=["What is this image?",
              types.Part.from_bytes(image.content, "image/jpeg")])

print(response.text)

# How to Use Gemini Thinking Model
# Note: Run this in a notebook or command line, not in a Python file
# pip install -U google-genai

GOOGLE_API_KEY = "YOUR_API_KEY"  # Replace with your actual API key

client = genai.Client(
    api_key=GOOGLE_API_KEY,
    http_options={'api_version':'v1alpha'},
)

# This example uses the new Google Genai SDK.
response = client.models.generate_content(
    model='gemini-2.0-flash-thinking-exp',
    contents='Explain how RLHF works in simple terms.',
)

print(response.text)

import os

# Set environment variables
os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'

# Generate content using the client
response = client.models.generate_content(
    model='gemini-2.0-flash-thinking-exp',
    contents='Explain how RLHF works in simple terms.')
print(response.text)

# The new Google Genai SDK provides the ability to create a multi-turn chat session 
# which is helpful to manage the state of a conversation.

# Define an async function for chat interactions
async def chat_interaction():
    client = genai.Client(api_key='GEMINI_API_KEY', http_options={'api_version':'v1alpha'})
    
    chat = client.aio.chats.create(
        model='gemini-2.0-flash-thinking-exp',
    )
    response = await chat.send_message('What is your name?')
    print(response.text)
    response = await chat.send_message('What did you just say before this?')
    print(response.text)

# Run the async chat function
if __name__ == "__main__":
    asyncio.run(chat_interaction())

# Replace placeholders like YOUR_GEMINI_API_KEY and file paths with your actual API keys and file locations. 
# This guide provides a structured approach to interacting with different models provided by the Gemini API, using Python as the programming language.

# Limitations
# The Flash Thinking model is an experimental model and has the following limitations:

# - Text and image input only
# - Text only output
# - No JSON mode or Search Grounding
# - Thoughts are only shown in Google AI Studio
